{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to torch nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxkwAI//ZNnq3wM51iM6Gx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiyewise/ML-with-PyTorch-Tutorials/blob/main/Introduction_to_torch_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za4lvNmz54eZ"
      },
      "source": [
        "### NN의 일반적인 학습 과정\r\n",
        "\r\n",
        "신경망의 일반적인 학습 과정\r\n",
        "* 학습 가능한 매개변수/weight 가지는 신경망을 정의\r\n",
        "* 데이터셋 입력, 입력을 신경망에서 전파\r\n",
        "   \r\n",
        "   입력을 받아 여러 계층에 차례로 전달한 후, 최종 출력 제공\r\n",
        "* 손실(loss) 계산\r\n",
        "* gradient backpropagation\r\n",
        "* update weight.\r\n",
        "   \r\n",
        "   new weight = weight - lr * gradient\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG0FApp_65RV",
        "outputId": "cc9b73b9-d789-4ffd-a1a0-78d68d71d6ff"
      },
      "source": [
        "# Example 1: CNN\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "      super(Net, self).__init__()\r\n",
        "      # input image channel 1 (one color), output channel 6, kernel size: 3*3 conv matrix\r\n",
        "      # convolutional kernel\r\n",
        "      self.conv1 = nn.Conv2d(1, 6, 3)\r\n",
        "      self.conv2 = nn.Conv2d(6, 16, 3)\r\n",
        "      # affine calcuations: y = Wx + b\r\n",
        "      self.fc1 = nn.Linear(16*6*6, 120) # we flatten the output of conv layers # 6*6: image size. - pooling 에서 cover\r\n",
        "      self.fc2 = nn.Linear(120, 84)\r\n",
        "      self.fc3 = nn.Linear (84, 10)\r\n",
        "  \r\n",
        "    def forward(self, x):\r\n",
        "      # max pooling on 2*2 size window\r\n",
        "      print((self.conv1(x)).size())\r\n",
        "      x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\r\n",
        "      print(x.size())\r\n",
        "      print((self.conv2(x)).size())\r\n",
        "      # if the size is in a n * n form, just write n\r\n",
        "      x = F.max_pool2d(F.relu(self.conv2(x)), 2)\r\n",
        "      print(x.size())\r\n",
        "      x = x.view(-1, self.num_flat_features(x)) # flatten the vector\r\n",
        "      x = F.relu(self.fc1(x))\r\n",
        "      x = F.relu(self.fc2(x))\r\n",
        "      x = self.fc3(x)\r\n",
        "      return x\r\n",
        "\r\n",
        "    def num_flat_features(self, x):\r\n",
        "      size = x.size()[1:] # x.size(): torch.Size([1, 16, 6, 6]) the first elements stands for batch\r\n",
        "      num_features = 1\r\n",
        "      for s in size:\r\n",
        "          num_features *= s\r\n",
        "      return num_features\r\n",
        "\r\n",
        "net = Net()\r\n",
        "print(net)\r\n",
        "input = torch.randn(1, 1, 32, 32) # input image size: 32 * 32\r\n",
        "net.forward(input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "torch.Size([1, 6, 30, 30])\n",
            "torch.Size([1, 6, 15, 15])\n",
            "torch.Size([1, 16, 13, 13])\n",
            "torch.Size([1, 16, 6, 6])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0808, -0.0263, -0.0605, -0.0469, -0.1509, -0.0195,  0.0944,  0.0197,\n",
              "         -0.0274, -0.1059]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Bb0p2dCkTG"
      },
      "source": [
        "### Example CNN 코드 설명\r\n",
        "\r\n",
        "##### **Image Size**\r\n",
        "1. input = torch.Size([1, 1, 32, 32): 1 batch, 1 color, 32x32 image\r\n",
        "\r\n",
        "2. output 6 channel, kernel size 3x3인 conv1 거치고 나면 32-3+1 인 30x30 image\r\n",
        "\r\n",
        "3. 2*2 size window의 max_pool2d 거치고 나면 30/2 = 15, 15x15 image\r\n",
        "\r\n",
        "4. 마찬가지로  kernel size 3x3인 conv2 거치고 나면 15-3+1 인 13x13 image\r\n",
        "\r\n",
        "5. 마찬가지로 2*2 size window의 max_pool2d 거치고 나면 13/2 = 6, 6x6 image\r\n",
        "\r\n",
        "##### **Flattening the vector**\r\n",
        "max_pool2d를 거치고 난 후 fully connected layer 통과하려면 2d vector가 1d vector 로 flatten 되어야 하는데, 이 부분을 계산하는 게 num_flat_features 함수이다.\r\n",
        "\r\n",
        "주어진 텐서, e.g. [1,16,6,6]에서 첫번째 인자인 1은 batch size를 나타내는 거니까 제외하고 나머지 16x6x6 길이로 flatten 시킨다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pUXe7katemP",
        "outputId": "80ac29b5-81b1-4d51-9900-78cd3e6e8c27"
      },
      "source": [
        "# parametes of the neural network\r\n",
        "params = list(net.parameters())\r\n",
        "# print(params)\r\n",
        "print(len(params))\r\n",
        "for i in range(len(params)):\r\n",
        "  print(params[i].size())\r\n",
        "\r\n",
        "# 10\r\n",
        "# torch.Size([6, 1, 3, 3])\r\n",
        "# torch.Size([6])\r\n",
        "# torch.Size([16, 6, 3, 3])\r\n",
        "# torch.Size([16])\r\n",
        "# torch.Size([120, 576])\r\n",
        "# torch.Size([120])\r\n",
        "# torch.Size([84, 120])\r\n",
        "# torch.Size([84])\r\n",
        "# torch.Size([10, 84])\r\n",
        "# torch.Size([10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "torch.Size([6, 1, 3, 3])\n",
            "torch.Size([6])\n",
            "torch.Size([16, 6, 3, 3])\n",
            "torch.Size([16])\n",
            "torch.Size([120, 576])\n",
            "torch.Size([120])\n",
            "torch.Size([84, 120])\n",
            "torch.Size([84])\n",
            "torch.Size([10, 84])\n",
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}