{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to torch nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwHHoZHPyGddH8Bao2VuPm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiyewise/ML-with-PyTorch-Tutorials/blob/main/Introduction_to_torch_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za4lvNmz54eZ"
      },
      "source": [
        "### NN의 일반적인 학습 과정\r\n",
        "\r\n",
        "신경망의 일반적인 학습 과정\r\n",
        "* 학습 가능한 매개변수/weight 가지는 신경망을 정의\r\n",
        "* 데이터셋 입력, 입력을 신경망에서 전파\r\n",
        "   \r\n",
        "   입력을 받아 여러 계층에 차례로 전달한 후, 최종 출력 제공\r\n",
        "* 손실(loss) 계산\r\n",
        "* gradient backpropagation\r\n",
        "* update weight.\r\n",
        "   \r\n",
        "   new weight = weight - lr * gradient\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG0FApp_65RV",
        "outputId": "92d8edb2-6094-4582-c863-591ad04c093f"
      },
      "source": [
        "# Example 1: CNN\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "      super(Net, self).__init__()\r\n",
        "      # input image channel 1 (one color), output channel 6, kernel size: 3*3 conv matrix\r\n",
        "      # convolutional kernel\r\n",
        "      self.conv1 = nn.Conv2d(1, 6, 3)\r\n",
        "      self.conv2 = nn.Conv2d(6, 16, 3)\r\n",
        "      # affine calcuations: y = Wx + b\r\n",
        "      self.fc1 = nn.Linear(16*6*6, 120) # we flatten the output of conv layers # 6*6: image size. - pooling 에서 cover\r\n",
        "      self.fc2 = nn.Linear(120, 84)\r\n",
        "      self.fc3 = nn.Linear (84, 10)\r\n",
        "  \r\n",
        "    def forward(self, x):\r\n",
        "      # max pooling on 2*2 size window\r\n",
        "    #   print((self.conv1(x)).size())\r\n",
        "      x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\r\n",
        "    #   print(x.size())\r\n",
        "    #   print((self.conv2(x)).size())\r\n",
        "      # if the size is in a n * n form, just write n\r\n",
        "      x = F.max_pool2d(F.relu(self.conv2(x)), 2)\r\n",
        "    #   print(x.size())\r\n",
        "      x = x.view(-1, self.num_flat_features(x)) # flatten the vector\r\n",
        "      x = F.relu(self.fc1(x))\r\n",
        "      x = F.relu(self.fc2(x))\r\n",
        "      x = self.fc3(x)\r\n",
        "      return x\r\n",
        "\r\n",
        "    def num_flat_features(self, x):\r\n",
        "      size = x.size()[1:] # x.size(): torch.Size([1, 16, 6, 6]) the first elements stands for batch\r\n",
        "      num_features = 1\r\n",
        "      for s in size:\r\n",
        "          num_features *= s\r\n",
        "      return num_features\r\n",
        "\r\n",
        "net = Net()\r\n",
        "print(net)\r\n",
        "input = torch.randn(1, 1, 32, 32) # input image size: 32 * 32\r\n",
        "net.forward(input)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0157, -0.0020,  0.0240, -0.1025,  0.0076, -0.0465,  0.0301,  0.0072,\n",
              "         -0.1387,  0.1185]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Bb0p2dCkTG"
      },
      "source": [
        "### Example CNN 코드 설명\r\n",
        "\r\n",
        "##### **Image Size**\r\n",
        "1. input = torch.Size([1, 1, 32, 32): 1 batch, 1 color, 32x32 image\r\n",
        "\r\n",
        "2. output 6 channel, kernel size 3x3인 conv1 거치고 나면 32-3+1 인 30x30 image\r\n",
        "\r\n",
        "3. 2*2 size window의 max_pool2d 거치고 나면 30/2 = 15, 15x15 image\r\n",
        "\r\n",
        "4. 마찬가지로  kernel size 3x3인 conv2 거치고 나면 15-3+1 인 13x13 image\r\n",
        "\r\n",
        "5. 마찬가지로 2*2 size window의 max_pool2d 거치고 나면 13/2 = 6, 6x6 image\r\n",
        "\r\n",
        "##### **Flattening the vector**\r\n",
        "max_pool2d를 거치고 난 후 fully connected layer 통과하려면 2d vector가 1d vector 로 flatten 되어야 하는데, 이 부분을 계산하는 게 num_flat_features 함수이다.\r\n",
        "\r\n",
        "주어진 텐서, e.g. [1,16,6,6]에서 첫번째 인자인 1은 batch size를 나타내는 거니까 제외하고 나머지 16x6x6 길이로 flatten 시킨다.\r\n",
        "\r\n",
        "#####  **Mini-Batches in Torch.nn**\r\n",
        "`torch.nn` 는 mini batch만 지원한다. 즉 torch.nn으로 들어오는 Input들은 모두 sample들의 Mini-batch 형태여야 한다. 위 코드의 경우, `nn.Conv2d`는 nSamples, nChannels, Height, Width 의 4D 텐서를 input으로 받을 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOrYFqKGcA_P"
      },
      "source": [
        "### Learning\n",
        "##### **Parameters**\n",
        "`net.parameters` return the learnable parameters of the given model.\n",
        "##### **Backpropagation**\n",
        "When the `forward` function is defined, the `backward` function is automatically defined for using `autograd`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pUXe7katemP",
        "outputId": "7b3ddd4f-8a43-4873-c94d-672426862e83"
      },
      "source": [
        "# parametes of the neural network\r\n",
        "params = list(net.parameters())\r\n",
        "# print(params)\r\n",
        "print(len(params))\r\n",
        "for i in range(len(params)):\r\n",
        "  print(params[i].size())\r\n",
        "\r\n",
        "# 10\r\n",
        "# torch.Size([6, 1, 3, 3])\r\n",
        "# torch.Size([6])\r\n",
        "# torch.Size([16, 6, 3, 3])\r\n",
        "# torch.Size([16])\r\n",
        "# torch.Size([120, 576])\r\n",
        "# torch.Size([120])\r\n",
        "# torch.Size([84, 120])\r\n",
        "# torch.Size([84])\r\n",
        "# torch.Size([10, 84])\r\n",
        "# torch.Size([10])\r\n",
        "\r\n",
        "# backpropagation with .backward\r\n",
        "input = torch.randn(1, 1, 32, 32)\r\n",
        "out = net(input)\r\n",
        "print(out)\r\n",
        "\r\n",
        "net.zero_grad() # zero the gradient buffers of all parameters\r\n",
        "out.backward(torch.randn(1, 10)) # backprop with random gradients"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "torch.Size([6, 1, 3, 3])\n",
            "torch.Size([6])\n",
            "torch.Size([16, 6, 3, 3])\n",
            "torch.Size([16])\n",
            "torch.Size([120, 576])\n",
            "torch.Size([120])\n",
            "torch.Size([84, 120])\n",
            "torch.Size([84])\n",
            "torch.Size([10, 84])\n",
            "torch.Size([10])\n",
            "tensor([[-0.0103,  0.0059,  0.0139, -0.1349, -0.0003, -0.0339,  0.0210,  0.0609,\n",
            "         -0.1260,  0.1202]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2CPmskIfTXR"
      },
      "source": [
        "### Loss \n",
        "A loss function takes the (output, target) pair as an input and calculate the difference between.\n",
        "\n",
        "Following `loss` in the backward direction, the graph of computations is as follows:\n",
        "```\n",
        "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
        "      -> view -> linear -> relu -> linear -> relu -> linear\n",
        "      -> MSELoss\n",
        "      -> loss\n",
        "```\n",
        "\n",
        "Therefore, when we call `loss.backward()`, the whole graph is differenciated with respect to the loss, and all tensors with `.grad` will have their `.grad` tensor accumulated.\n",
        "\n",
        "### Backprop\n",
        "1. First clear the existing gradients\n",
        "2. call `loss.backward()`\n",
        "\n",
        "### Updating the Weights\n",
        "The simplest update is the Stochastic Gradient Descent(SGD):\n",
        "```\n",
        "weight = weight - lr*gradient\n",
        "```\n",
        "However, other optimizers are also possible from the package `torch.optim`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvx97dBibVUY",
        "outputId": "cf6243fc-2a67-4b6e-8bed-d36a7bb0e383"
      },
      "source": [
        "# Loss, backpropagation, and optimization\n",
        "\n",
        "# Loss\n",
        "output = net(input)\n",
        "target = torch.randn(10) # torch.Size([10])\n",
        "print(target.size())\n",
        "target = target.view(1, -1) # torch.Size([1, 10])\n",
        "print(target.size())\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "\n",
        "# Backprop & optimization\n",
        "learning_rate = 0.01\n",
        "import torch.optim as optim\n",
        "\n",
        "# create optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in the training loop\n",
        "optimizer.zero_grad() # initialize\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step() # Update is done here"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10])\n",
            "torch.Size([1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}