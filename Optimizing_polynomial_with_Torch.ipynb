{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimizing polynomial with Torch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNdIuD7mc/etw5gMNOjE7GB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiyewise/ML-with-PyTorch-Tutorials/blob/main/Optimizing_polynomial_with_Torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbV5hVZ_bx__"
      },
      "source": [
        "### Learning PyTorch with Examples: Optimizing Polynomial\n",
        "앞에서 `torch.nn`, `autograd` 와 같은 개념들을 이미 한번 돌아보았기 때문에 여기서는 이 개념들을 간단한 예시로 복습하고자 한다. 여기서는 sin함수를 근사한 $ax^3 + bx^2 + cx + d$ 와 같은 3차원 polynomial의 계수를 optimization을 통해 찾아보고자 한다.\n",
        "\n",
        "\n",
        "1. with `Torch.Tensor` and manual gradient descent\n",
        "2. defining new `autograd `(with `torch.autograd.Function`)\n",
        "3. `nn.Module` & without `optimizer`\n",
        "4. `nn.Module` & with `optimizer`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "xHEpKc6idNA3",
        "outputId": "9018ccfd-a57c-4fe9-a8a1-84e55369b2a8"
      },
      "source": [
        "# Example 1: Implementation with Torch.Tensor and manual gradient descent\n",
        "\n",
        "import torch\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
        "\n",
        "\"\"\"\n",
        "Create Tensors to hold input and outputs.\n",
        "By default, requires_grad=False, which indicates that we do not need to\n",
        "compute gradients with respect to these Tensors during the backward pass.\n",
        "We do not have to compute gradients as the x and y values are for training, \n",
        "and they are not targeted for optimization\n",
        "\"\"\"\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "\"\"\"\n",
        "Create random Tensors for weights. For a third order polynomial, we need\n",
        "4 weights: y = a + b x + c x^2 + d x^3\n",
        "\"\"\"\n",
        "\n",
        "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "\n",
        "# plot the result\n",
        "loss_index = []\n",
        "loss_y = []\n",
        "\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y using operations on Tensors.\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute loss with y, which is the calculated sin value\n",
        "    loss = (y_pred - y).pow(2).sum() # form of Tensor[1,]\n",
        "    if t % 50 == 0:\n",
        "        # print(t, loss.item())\n",
        "        loss_index.append(t) \n",
        "        loss_y.append(loss.item()) #loss.item() changes into a scalar value\n",
        "\n",
        "    loss.backward() # computing the gradients\n",
        "\n",
        "    \"\"\"\n",
        "    Instead of using autograd,\n",
        "    Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
        "    This is because weights have requires_grad=True, \n",
        "    but we don't need to track this in autograd in autograd.\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        a -= learning_rate * a.grad\n",
        "        b -= learning_rate * b.grad\n",
        "        c -= learning_rate * c.grad\n",
        "        d -= learning_rate * d.grad\n",
        "        \n",
        "        # if t % 100 == 99:\n",
        "        #     print(f\"Under optimization: a: {a} b: {b} c: {c} d: {d}\")\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        a.grad = None\n",
        "        b.grad = None\n",
        "        c.grad = None\n",
        "        d.grad = None\n",
        "\n",
        "# plot and print the result\n",
        "plt.plot(loss_index, loss_y)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Loss: {loss.item()}\")\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 10.573115348815918\n",
            "Result: y = -0.038325466215610504 + 0.836235761642456 x + 0.006611780263483524 x^2 + -0.09041371941566467 x^3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25_Qs18tgwgG"
      },
      "source": [
        "### Inheriting the `torch.autograd.Function`\n",
        "\n",
        "By inheriting the `torch.autograd.Function` class, we can define the `forward` and `backward` functions as follows:\n",
        "\n",
        "```Python\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        In the forward pass we receive a Tensor containing the input and return\n",
        "        a Tensor containing the output. ctx is a context object that can be used\n",
        "        to stash information for backward computation. You can cache arbitrary\n",
        "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(input)\n",
        "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
        "        with respect to the output, and we need to compute the gradient of the loss\n",
        "        with respect to the input.\n",
        "        \"\"\"\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
        "```\n",
        "\n",
        "As we are using a 3 degree polynomial,we define our model as $y=a+bP3(c+dx)$ instead of $y=a+bx+cx2+dx3$, where $P3(x)=12(5x3−3x)$ is the Legendre polynomial of degree three. The model and its gradient are defined in the `forward` and `backward` function, respectively.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SksWaCKFhTOd",
        "outputId": "dc7fc67c-d2cd-49a2-c79f-61df60dfc347"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "class LegendrePolynomial3(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    We can implement our own custom autograd Functions by subclassing\n",
        "    torch.autograd.Function and implementing the forward and backward passes\n",
        "    which operate on Tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        In the forward pass we receive a Tensor containing the input and return\n",
        "        a Tensor containing the output. ctx is a context object that can be used\n",
        "        to stash information for backward computation. You can cache arbitrary\n",
        "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(input)\n",
        "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
        "        with respect to the output, and we need to compute the gradient of the loss\n",
        "        with respect to the input.\n",
        "        \"\"\"\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
        "\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
        "\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "\"\"\"\n",
        "Create random Tensors for weights. For this example, we need\n",
        "4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n",
        "not too far from the correct result to ensure convergence.\n",
        "Setting requires_grad=True indicates that we want to compute gradients with\n",
        "respect to these Tensors during the backward pass.\n",
        "\"\"\"\n",
        "\n",
        "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
        "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
        "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
        "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 5e-6\n",
        "for t in range(2000):\n",
        "\n",
        "    \"\"\"\n",
        "    To apply our Function, we use Function.apply method. We alias this as 'P3'.\n",
        "    \"\"\"\n",
        "    # NOTE: not apply() as this is not \"calling\" the function\n",
        "    P3 = LegendrePolynomial3.apply\n",
        "\n",
        "    \"\"\"\n",
        "    Forward pass: compute predicted y using operations\n",
        "    we compute P3 using our custom autograd operation.\n",
        "    Compare this with the previous example, where \n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "    \"\"\"\n",
        "\n",
        "    # same as y = a + b * LegendrePolynomial3.apply(c + d * x)\n",
        "    y_pred = a + b * P3(c + d * x) \n",
        "    loss = (y_pred - y).pow(2).sum() # loss calculated\n",
        "\n",
        "    # plot and print\n",
        "    # if t % 100 == 99:\n",
        "    #     print(t, loss.item())\n",
        "\n",
        "    \"\"\"\n",
        "    Customized backward function called here for calculating gradients\n",
        "    \"\"\"\n",
        "    loss.backward() \n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    with torch.no_grad():\n",
        "        a -= learning_rate * a.grad\n",
        "        b -= learning_rate * b.grad\n",
        "        c -= learning_rate * c.grad\n",
        "        d -= learning_rate * d.grad\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        a.grad = None\n",
        "        b.grad = None\n",
        "        c.grad = None\n",
        "        d.grad = None\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Result: y = -5.394172664097141e-09 + -2.208526849746704 * P3(1.367587154632588e-09 + 0.2554861009120941 x)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCcnEGteihu3"
      },
      "source": [
        "### Using `Torch.nn` for Polynomials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RLLwkqOik-d",
        "outputId": "3f76fd8e-90b9-49f7-e1aa-a1197084d414"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "# Create Tensors to hold input and outputs.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "\"\"\"\n",
        "For this example, the output y is a linear function of (x, x^2, x^3), so\n",
        "we can consider it as a linear layer neural network. Let's prepare the\n",
        "tensor (x, x^2, x^3).\n",
        "\"\"\"\n",
        "p = torch.tensor([1, 2, 3])\n",
        "xx = x.unsqueeze(-1).pow(p)\n",
        "\n",
        "\"\"\"\n",
        "In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
        "(3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
        "of shape (2000, 3).\n",
        "\n",
        "In [1]: x.size()\n",
        "Out[1]: torch.Size([2000])\n",
        "\n",
        "In [2]: xx.size()\n",
        "Out[2]: torch.Size([2000, 3])\n",
        "\n",
        "In [3]: x.unsqueeze(-1).size()\n",
        "Out[3]: torch.Size([2000, 1])\n",
        "\n",
        "In [4]: x.unsqueeze(-1)\n",
        "Out[4]: \n",
        "tensor([[-3.1416],\n",
        "        [-3.1384],\n",
        "        [-3.1353],\n",
        "        ...,\n",
        "        [ 3.1353],\n",
        "        [ 3.1384],\n",
        "        [ 3.1416]])\n",
        "\n",
        "In [5]: xx\n",
        "Out[5]: \n",
        "tensor([[ -3.1416,   9.8696, -31.0063],\n",
        "        [ -3.1384,   9.8499, -30.9133],\n",
        "        [ -3.1353,   9.8301, -30.8205],\n",
        "        ...,\n",
        "        [  3.1353,   9.8301,  30.8205],\n",
        "        [  3.1384,   9.8499,  30.9133],\n",
        "        [  3.1416,   9.8696,  31.0063]])\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
        "is a Module which contains other Modules, and applies them in sequence to\n",
        "produce its output. The Linear Module computes output from input using a\n",
        "linear function, and holds internal Tensors for its weight and bias.\n",
        "The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
        "to match the shape of `y`.\n",
        "\"\"\"\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(3, 1),\n",
        "    torch.nn.Flatten(0, 1)\n",
        "\n",
        "    # without torch.nn.Flatten(0,1): y_pred.size() is [2000,1]\n",
        "    # with Flatten, size() is [2000]   \n",
        "\n",
        ")\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "\n",
        "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
        "    # override the __call__ operator so you can call them like functions. When\n",
        "    # doing so you pass a Tensor of input data to the Module and it produces\n",
        "    # a Tensor of output data.\n",
        "\n",
        "    y_pred = model(xx)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "\n",
        "    # if t % 100 == 99:\n",
        "    #     print(t, loss.item())\n",
        "\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
        "    # we can access its gradients like we did before.\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad\n",
        "\n",
        "# You can access the first layer of `model` like accessing the first item of a list\n",
        "linear_layer = model[0]\n",
        "\n",
        "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
        "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Result: y = 0.008108377456665039 + 0.8423072099685669 x + -0.0013988293940201402 x^2 + -0.09127733111381531 x^3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbU8i0LTnXwr"
      },
      "source": [
        "### Customizing `nn.Modules` by Inheritance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAm81b30nb0N"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "class Polynomial3(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate four parameters and assign them as\n",
        "        member parameters.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.a = torch.nn.Parameter(torch.randn(()))\n",
        "        self.b = torch.nn.Parameter(torch.randn(()))\n",
        "        self.c = torch.nn.Parameter(torch.randn(()))\n",
        "        self.d = torch.nn.Parameter(torch.randn(()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
        "\n",
        "    def string(self):\n",
        "        \"\"\"\n",
        "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
        "        \"\"\"\n",
        "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
        "\n",
        "\n",
        "# Create Tensors to hold input and outputs.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Construct our model by instantiating the class defined above\n",
        "model = Polynomial3()\n",
        "\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
        "\n",
        "for t in range(2000):\n",
        "    \n",
        "    y_pred = model(x) # Forward pass\n",
        "    loss = criterion(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f'Result: {model.string()}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}