{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to torch autograd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlcAuQW9aMaCUttmlwOyTB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiyewise/ML-with-PyTorch-Tutorials/blob/main/Introduction_to_torch_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcDFVlInEec6"
      },
      "source": [
        "### Tensor\n",
        "\n",
        "`requires_grad` 속성을 True로 설정하면, 텐서에서 이루어지는 모든 연산들이 추적되고, 계산이 완료된 후 `.backward()` 를 호출하면 모든 gradient를 자동으로 계산할 수 있고, 이 gradient는 텐서의 `.grad` 속성에 누적된다.\n",
        "\n",
        "텐서에서 이루어지는 연산들이 추적되는 걸 중단하려면, `.detach()` 를 호출해서 연산 기록으로부터 detach, 즉 분리할 수 있다.\n",
        "\n",
        "### Function\n",
        "\n",
        "텐서와 function은 서로 연결되어 있고, 연산 과정은 acyclic graph를 생성한다.\n",
        "\n",
        "각 텐서는 `.grad_fn()` 속성을 가지고 있는데, 이는 텐서를 생성한 function을 참조한다. 단, 사용자가 만든 텐서는 예외 (이 텐서는 사용자가 만들었지, function이 만든 게 아니니까. 이때 `.grad_fn()`은 0이다.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orSjmlOEEhyX",
        "outputId": "bef31677-1a06-4463-f378-c2444d46afe5"
      },
      "source": [
        "# Example 1: Create Tensor\n",
        "import torch\n",
        "# x = torch.ones(2, 2, requires_grad=True)\n",
        "x = torch.tensor([[2, 2], [2, 2]], dtype=torch.float, requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2., 2.],\n",
            "        [2., 2.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWrUkRqFEwUH",
        "outputId": "e435f04e-bf76-4488-e142-1f7e89d34c61"
      },
      "source": [
        "# Example 1: Tensor에 연산 수행하기\n",
        "y = x + 2\n",
        "print(y)\n",
        "# print grad_fn of y\n",
        "print(y.grad_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4., 4.],\n",
            "        [4., 4.]], grad_fn=<AddBackward0>)\n",
            "<AddBackward0 object at 0x7ff713c75c88>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i9miAF0E6c1"
      },
      "source": [
        "`y`는 연산의 결과로 만들어진 텐서이기 때문에 `grad_fn`을 가진다. `grad_fn`은 텐서를 만든 function을 참조하는데, 여기서는 덧셈으로 만들어졌으므로 `AddBackward0` function이 참조되었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykDRBMXdFOGk",
        "outputId": "de0be4af-7d60-4371-803d-a4cf1a6aa6bc"
      },
      "source": [
        "# Example 1: 다른 연산 수행\n",
        "z = y*y*3 #3(x+2)^2\n",
        "out = z.mean()\n",
        "\n",
        "print(z, out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[48., 48.],\n",
            "        [48., 48.]], grad_fn=<MulBackward0>) tensor(48., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgSNRgvFGjlZ",
        "outputId": "e8c126ef-723e-4075-da6f-97777f146c14"
      },
      "source": [
        "# Example 2: .requires_grad_ & detach\n",
        "a = torch.randn(2,2)\n",
        "a = ((a+3)/(a-1))\n",
        "print(a)\n",
        "print(a.requires_grad)\n",
        "print(\"-------requires_grad_ to True--------\")\n",
        "a.requires_grad_(True)\n",
        "print(a.requires_grad)\n",
        "b = (a*a).sum()\n",
        "print(b)\n",
        "print(b.grad_fn)\n",
        "print(b.detach())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-2.0617, -0.6829],\n",
            "        [ 6.9749, -0.3832]])\n",
            "False\n",
            "-------requires_grad_ to True--------\n",
            "True\n",
            "tensor(53.5131, grad_fn=<SumBackward0>)\n",
            "<SumBackward0 object at 0x7ff714520a58>\n",
            "tensor(53.5131)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91yVOi_7JdWk"
      },
      "source": [
        "### Gradient\n",
        "도함수를 계산하기 위해서는 텐서의 `.backward()` 를 call 하면 된다.\n",
        "gradient를 계산하고자 하는 텐서가 스칼라라면 (변수가 1개밖에 없다면) `backward`에 따로 텐서 인자를 지정해줄 필요가 없다. 그러나 텐서가 여러 개의 변수를 가질 때는 텐서의 인자를 지정해 주어야 한다. Example 3 참고.\n",
        "\n",
        "### Vector-Jacobian\n",
        "vector-jacobian에 대한 설명은 [튜토리얼 링크](https://tutorials.pytorch.kr/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py) 참고.\n",
        "결과적으로 vector-jacobian은 x vector 즉 (x1, x2, ... xn)에 대한 함수 l의 기울기가 됨을 알 수 있다. \n",
        "`torch.autograd`는 이러한 vector-jacobian 곱을 계산하는 엔진.\n",
        "\n",
        "* [링크](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)에서 가져온 설명인데 가장 잘 이해되는 설명이라 붙여놓음.\n",
        "\"If `x` is a Tensor that has `x.requires_grad=True` then `x.grad` is another Tensor holding the gradient of `x` with respect to some scalar value.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u077JwecVmrN",
        "outputId": "b8af057d-f73a-48b5-a170-9b2a560a3cdf"
      },
      "source": [
        "# Example 3: .backward() when tensor is a scalar\n",
        "# out.backward()\n",
        "# print(x.grad) # recall example 1\n",
        "\n",
        "# Example 3: vector-jacobian by torch.autograd\n",
        "x = torch.ones(3, requires_grad = True)\n",
        "y = x * 2\n",
        "print(x)\n",
        "while y.data.norm() < 1000:\n",
        "  y = y * 2\n",
        "print(y)\n",
        "\n",
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
        "y.backward(v) \n",
        "# y.backward(x) \n",
        "print(x.grad) # y.grad would not work as it is not a leaf tensor \n",
        "\n",
        "# Example 3: scalar prod. (when tensor is a scalar) \n",
        "print(\"---------Scalar Prod-----------\")\n",
        "x2 = torch.ones(3, requires_grad = True)\n",
        "y2 = x2 * 2\n",
        "print(x2)\n",
        "while y2.data.norm() < 1000:\n",
        "  y2 = y2 * 2\n",
        "print(y2)\n",
        "\n",
        "out = y2.mean()\n",
        "out.backward() # default: gradient calculation on values of x\n",
        "# out.backward(torch.tensor(0.1, dtype=float)) # 0.1 is passed to the gradient\n",
        "print(x2.grad) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1.], requires_grad=True)\n",
            "tensor([1024., 1024., 1024.], grad_fn=<MulBackward0>)\n",
            "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n",
            "---------Scalar Prod-----------\n",
            "tensor([1., 1., 1.], requires_grad=True)\n",
            "tensor([1024., 1024., 1024.], grad_fn=<MulBackward0>)\n",
            "tensor([341.3333, 341.3333, 341.3333])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gMmt5UjXAVX",
        "outputId": "82c81151-ed30-43e4-f174-339565dbf51d"
      },
      "source": [
        "# Example 4: torch.no_grad() & detach\n",
        "print(x.requires_grad)\n",
        "print((x**2).requires_grad)\n",
        "\n",
        "# wrap the code block with no_grad\n",
        "with torch.no_grad():\n",
        "  print((x**2).requires_grad)\n",
        "\n",
        "# detach: detach creates a new tensor with same content but different require_grad\n",
        "y = x.detach()\n",
        "print(y.requires_grad) # different require_grad\n",
        "print(x.eq(y).all()) # same value"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "False\n",
            "False\n",
            "tensor(True)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}